{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vectorize","text":"pg_vectorize: a VectorDB on Postgres <p>A Postgres server and extension that automates the transformation and orchestration of text to embeddings and provides hooks into the most popular LLMs. This allows you to do get up and running and automate maintenance for vector search, full text search, and hybrid search, which enables you to quickly build RAG and search engines on Postgres.</p> <p>This project relies heavily on the work by pgvector for vector similarity search, pgmq for orchestration in background workers, and SentenceTransformers.</p> <p></p> <p>API Documentation: https://chuckhend.github.io/pg_vectorize/</p> <p>Source: https://github.com/tembo-io/pg_vectorize</p>"},{"location":"#overview","title":"Overview","text":"<p>pg_vectorize provides two ways to add semantic, full text, and hybrid search to any Postgres making it easy to build retrieval-augmented generation (RAG) on Postgres. This project provides an external server only implementation and SQL experience via a Postgres extension.</p> <p>Modes at a glance:</p> <ul> <li>HTTP server (recommended for managed DBs): run a standalone service that connects to Postgres and exposes a REST API (POST /api/v1/table, GET /api/v1/search).</li> <li>Postgres extension (SQL): install the extension into Postgres and use SQL functions like <code>vectorize.table()</code> and <code>vectorize.search()</code> (requires filesystem access to Postgres; see ./extension/README.md).</li> </ul>"},{"location":"#quick-start-http-server","title":"Quick start \u2014 HTTP server","text":"<p>Run Postgres and the HTTP servers locally using docker compose:</p> <pre><code># runs Postgres, the embeddings server, and the management API\ndocker compose up -d\n</code></pre> <p>Load the example dataset into Postgres (optional):</p> <pre><code>psql postgres://postgres:postgres@localhost:5432/postgres -f server/sql/example.sql\n</code></pre> <pre><code>CREATE TABLE\nINSERT 0 40\n</code></pre> <p>Create an embedding job via the HTTP API. This generates embeddings for the existing data and continuously watches for updates or new data:</p> <pre><code>curl -X POST http://localhost:8080/api/v1/table -d '{\n        \"job_name\": \"my_job\",\n        \"src_table\": \"my_products\",\n        \"src_schema\": \"public\",\n        \"src_columns\": [\"product_name\", \"description\"],\n        \"primary_key\": \"product_id\",\n        \"update_time_col\": \"updated_at\",\n        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n    }' -H \"Content-Type: application/json\"\n</code></pre> <pre><code>{\"id\":\"16b80184-2e8e-4ee6-b7e2-1a068ff4b314\"}\n</code></pre> <p>Search using the HTTP API:</p> <pre><code>curl -X GET \"http://localhost:8080/api/v1/search?job_name=my_job&amp;query=camping%20backpack&amp;limit=1\" | jq .\n</code></pre> <pre><code>[\n  {\n    \"description\": \"Storage solution for carrying personal items on ones back\",\n    \"fts_rank\": 1,\n    \"price\": 45.0,\n    \"product_category\": \"accessories\",\n    \"product_id\": 6,\n    \"product_name\": \"Backpack\",\n    \"rrf_score\": 0.03278688524590164,\n    \"semantic_rank\": 1,\n    \"similarity_score\": 0.6296013593673706,\n    \"updated_at\": \"2025-10-04T14:45:16.152526+00:00\"\n  }\n]\n</code></pre>"},{"location":"#which-should-i-pick","title":"Which should I pick?","text":"<ul> <li>Use the HTTP server when your Postgres is managed (RDS, Cloud SQL, etc.) or you cannot install extensions. It requires only that <code>pgvector</code> is available in the database. You the HTTP services separately.</li> <li>Use Postgres extension when you self-host Postgres and can install extensions. This provides an in-database experience and direct SQL APIs for vectorization and RAG.</li> </ul> <p>If you want hands-on SQL examples or to install the extension into Postgres, see <code>./extension/README.md</code>. For full HTTP API docs and deployment notes, see <code>./server/README.md</code>.</p> <p>For contribution guidelines see <code>CONTRIBUTING.md</code> in the repo root.</p>"},{"location":"extension/","title":"Index","text":"pg_vectorize: a VectorDB for Postgres <p>A Postgres extension that automates the transformation and orchestration of text to embeddings and provides hooks into the most popular LLMs. This allows you to do vector search and build LLM applications on existing data with as little as two function calls.</p> <p>This project relies heavily on the work by pgvector for vector similarity search, pgmq for orchestration in background workers, and SentenceTransformers.</p> <p></p> <p>API Documentation: https://chuckhend.github.io/pg_vectorize/</p> <p>Source: https://github.com/ChuckHend/pg_vectorize</p>"},{"location":"extension/#features","title":"Features","text":"<ul> <li>Workflows for both vector search and RAG</li> <li>Integrations with OpenAI's embeddings and Text-Generation endpoints and a self-hosted container for running Hugging Face Sentence-Transformers</li> <li>Automated creation of Postgres triggers to keep your embeddings up to date</li> <li>High level API - one function to initialize embeddings transformations, and another function to search</li> </ul>"},{"location":"extension/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Features</li> <li>Table of Contents</li> <li>Installation</li> <li>Vector Search Example</li> <li>RAG Example</li> <li>Updating Embeddings</li> <li>Directly Interact with LLMs</li> <li>Importing Pre-existing Embeddings</li> <li>Creating a Table from Existing Embeddings</li> </ul>"},{"location":"extension/#installation","title":"Installation","text":"<p>The fastest way to get started is by using docker compose.</p> <pre><code>docker compose up -d\n</code></pre> <p>Then connect to Postgres:</p> <pre><code>docker compose exec -it postgres psql\n</code></pre> <p>Enable the extension and its dependencies</p> <pre><code>CREATE EXTENSION vectorize CASCADE;\n</code></pre> Install into an existing Postgres instance  If you're installing in an existing Postgres instance, you will need the following dependencies:  Rust:  - [pgrx toolchain](https://github.com/pgcentralfoundation/pgrx)  Postgres Extensions:  - [pg_cron](https://github.com/citusdata/pg_cron) ^1.5 - [pgmq](https://github.com/pgmq/pgmq) ^1 - [pgvector](https://github.com/pgvector/pgvector) ^0.5.0  Then set the following either in postgresql.conf or as a configuration parameter:  <pre><code>-- requires restart of Postgres\nalter system set shared_preload_libraries = 'vectorize,pg_cron';\nalter system set cron.database_name = 'postgres';\n</code></pre>  And if you're running the vector-serve container, set the following url as a configuration parameter in Postgres.  The host may need to change from `localhost` to something else depending on where you are running the container.  <pre><code>alter system set vectorize.embedding_service_url = 'http://localhost:3000/v1';\n\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"extension/#vector-search-example","title":"Vector Search Example","text":"<p>Text-to-embedding transformation can be done with either Hugging Face's Sentence-Transformers or OpenAI's embeddings. The following examples use Hugging Face's Sentence-Transformers. See the project documentation for OpenAI examples.</p> <p>Follow the installation steps if you haven't already.</p> <p>Setup a products table. Copy from the example data provided by the extension.</p> <pre><code>CREATE TABLE products (LIKE vectorize.example_products INCLUDING ALL);\nINSERT INTO products SELECT * FROM vectorize.example_products;\n</code></pre> <pre><code>SELECT * FROM products limit 2;\n</code></pre> <pre><code> product_id | product_name |                      description                       |        last_updated_at        \n------------+--------------+--------------------------------------------------------+-------------------------------\n          1 | Pencil       | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05\n          2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics    | 2023-07-26 17:20:43.639351-05\n</code></pre> <p>Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description).</p> <pre><code>SELECT vectorize.table(\n    job_name    =&gt; 'product_search_hf',\n    relation    =&gt; 'products',\n    primary_key =&gt; 'product_id',\n    columns     =&gt; ARRAY['product_name', 'description'],\n    transformer =&gt; 'sentence-transformers/all-MiniLM-L6-v2',\n    schedule    =&gt; 'realtime'\n);\n</code></pre> <p>This adds a new column to your table, in our case it is named <code>product_search_embeddings</code>, then populates that data with the transformed embeddings from the <code>product_name</code> and <code>description</code> columns.</p> <p>Then search,</p> <pre><code>SELECT * FROM vectorize.search(\n    job_name        =&gt; 'product_search_hf',\n    query           =&gt; 'accessories for mobile devices',\n    return_columns  =&gt; ARRAY['product_id', 'product_name'],\n    num_results     =&gt; 3\n);\n</code></pre> <pre><code>                                       search_results                                        \n---------------------------------------------------------------------------------------------\n {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8147814132322894}\n {\"product_id\": 6, \"product_name\": \"Backpack\", \"similarity_score\": 0.7743061352550308}\n {\"product_id\": 11, \"product_name\": \"Stylus Pen\", \"similarity_score\": 0.7709902653575383}\n</code></pre>"},{"location":"extension/#rag-example","title":"RAG Example","text":"<p>Ask raw text questions of the example  <code>products</code> dataset and get chat responses from an OpenAI LLM.</p> <p>Follow the installation steps if you haven't already.</p> <p>Set the OpenAI API key, this is required to for use with OpenAI's chat-completion models.</p> <pre><code>ALTER SYSTEM SET vectorize.openai_key TO '&lt;your api key&gt;';\nSELECT pg_reload_conf();\n</code></pre> <p>Create an example table if it does not already exist.</p> <pre><code>CREATE TABLE products (LIKE vectorize.example_products INCLUDING ALL);\nINSERT INTO products SELECT * FROM vectorize.example_products;\n</code></pre> <p>Initialize a table for RAG. We'll use an open source Sentence Transformer to generate embeddings.</p> <p>Create a new column that we want to use as the context. In this case, we'll concatenate both <code>product_name</code> and <code>description</code>.</p> <pre><code>ALTER TABLE products\nADD COLUMN context TEXT GENERATED ALWAYS AS (product_name || ': ' || description) STORED;\n</code></pre> <p>Initialize the RAG project.  We'll use the <code>openai/text-embedding-3-small</code> model to generate embeddings on our source documents.</p> <pre><code>SELECT vectorize.table(\n    job_name    =&gt; 'product_chat',\n    relation    =&gt; 'products',\n    primary_key =&gt; 'product_id',\n    columns     =&gt; ARRAY['context'],\n    transformer =&gt; 'openai/text-embedding-3-small',\n    schedule    =&gt; 'realtime'\n);\n</code></pre> <p>Now we can ask questions of the <code>products</code> table and get responses from the <code>product_chat</code> agent using the <code>openai/gpt-3.5-turbo</code> generative model.</p> <pre><code>SELECT vectorize.rag(\n    job_name    =&gt; 'product_chat',\n    query       =&gt; 'What is a pencil?',\n    chat_model  =&gt; 'openai/gpt-3.5-turbo'\n) -&gt; 'chat_response';\n</code></pre> <pre><code>\"A pencil is an item that is commonly used for writing and is known to be most effective on paper.\"\n</code></pre> <p>And to use a locally hosted Ollama service, change the <code>chat_model</code> parameter:</p> <pre><code>SELECT vectorize.rag(\n    job_name    =&gt; 'product_chat',\n    query       =&gt; 'What is a pencil?',\n    chat_model  =&gt; 'ollama/wizardlm2:7b'\n) -&gt; 'chat_response';\n</code></pre> <pre><code>\" A pencil is a writing instrument that consists of a solid or gelignola wood core, known as the \\\"lead,\\\" encased in a cylindrical piece of breakable material (traditionally wood or plastic), which serves as the body of the pencil. The tip of the body is tapered to a point for writing, and it can mark paper with the imprint of the lead. When used on a sheet of paper, the combination of the pencil's lead and the paper creates a visible mark that is distinct from unmarked areas of the paper. Pencils are particularly well-suited for writing on paper, as they allow for precise control over the marks made.\"\n</code></pre> <p>:bulb: Note that the <code>-&gt; 'chat_response'</code> addition selects for that field of the JSON object output. Removing it will show the full JSON object, including information on which documents were included in the contextual prompt.</p>"},{"location":"extension/#updating-embeddings","title":"Updating Embeddings","text":"<p>When the source text data is updated, how and when the embeddings are updated is determined by the value set to the <code>schedule</code> parameter in <code>vectorize.table</code>.</p> <p>The default behavior is <code>schedule =&gt; '* * * * *'</code>, which means the background worker process checks for changes every minute, and updates the embeddings accordingly. This method requires setting the <code>updated_at_col</code> value to point to a colum on the table indicating the time that the input text columns were last changed. <code>schedule</code> can be set to any cron-like value.</p> <p>Alternatively, <code>schedule =&gt; 'realtime</code> creates triggers on the source table and updates embeddings anytime new records are inserted to the source table or existing records are updated.</p> <p>Statements below would will result in new embeddings being generated either immediately (<code>schedule =&gt; 'realtime'</code>) or within the cron schedule set in the <code>schedule</code> parameter.</p> <pre><code>INSERT INTO products (product_id, product_name, description, product_category, price)\nVALUES (12345, 'pizza', 'dish of Italian origin consisting of a flattened disk of bread', 'food', 5.99);\n\nUPDATE products\nSET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting'\nWHERE product_name = 'Hammock';\n</code></pre>"},{"location":"extension/#directly-interact-with-llms","title":"Directly Interact with LLMs","text":"<p>Sometimes you want more control over the handling of embeddings.  For those situations you can directly call various LLM providers using SQL:</p> <p>For text generation:</p> <pre><code>select vectorize.generate(\n  input =&gt; 'Tell me the difference between a cat and a dog in 1 sentence',\n  model =&gt; 'openai/gpt-4o'\n);\n</code></pre> <pre><code>                                                 generate                                                  \n-----------------------------------------------------------------------------------------------------------\n Cats are generally more independent and solitary, while dogs tend to be more social and loyal companions.\n(1 row)\n</code></pre> <p>And for embedding generation:</p> <pre><code>select vectorize.encode(\n  input =&gt; 'Tell me the difference between a cat and a dog in 1 sentence',\n  model =&gt; 'openai/text-embedding-3-large'\n);\n</code></pre> <pre><code>{0.0028769304,-0.005826319,-0.0035932811, ...}\n</code></pre>"},{"location":"extension/#importing-pre-existing-embeddings","title":"Importing Pre-existing Embeddings","text":"<p>If you have already computed embeddings using a compatible model (e.g., using Sentence-Transformers directly), you can import these into pg_vectorize without recomputation:</p> <pre><code>-- First create the vectorize project\nSELECT vectorize.table(\n    job_name    =&gt; 'my_search',\n    relation    =&gt; 'my_table',\n    primary_key =&gt; 'id',\n    columns     =&gt; ARRAY['content'],\n    transformer =&gt; 'sentence-transformers/all-MiniLM-L6-v2'\n);\n\n-- Then import your pre-computed embeddings\nSELECT vectorize.import_embeddings(\n    job_name            =&gt; 'my_search',\n    src_table           =&gt; 'my_embeddings_table',\n    src_primary_key     =&gt; 'id',\n    src_embeddings_col  =&gt; 'embedding'\n);\n</code></pre> <p>The embeddings must match the dimensions of the specified transformer model. For example, 'sentence-transformers/all-MiniLM-L6-v2' expects 384-dimensional vectors.</p>"},{"location":"extension/#creating-a-table-from-existing-embeddings","title":"Creating a Table from Existing Embeddings","text":"<p>If you have already computed embeddings using a compatible model, you can create a new vectorize table directly from them:</p> <pre><code>-- Create a vectorize table from existing embeddings\nSELECT vectorize.table_from(\n    relation =&gt; 'my_table',\n    columns =&gt; ARRAY['content'],\n    job_name =&gt; 'my_search',\n    primary_key =&gt; 'id',\n    src_table =&gt; 'my_embeddings_table',\n    src_primary_key =&gt; 'id',\n    src_embeddings_col =&gt; 'embedding',\n    transformer =&gt; 'sentence-transformers/all-MiniLM-L6-v2'\n);\n</code></pre> <p>The embeddings must match the dimensions of the specified transformer model. This approach ensures your pre-computed embeddings are properly imported before any automatic updates are enabled.</p>"},{"location":"extension/#contributing","title":"Contributing","text":"<p>We welcome contributions from the community! If you're interested in contributing to <code>pg_vectorize</code>, please check out our Contributing Guide. You can also open an issue.</p>"},{"location":"extension/configuration/","title":"Configuring pg_vectorize","text":""},{"location":"extension/configuration/#changing-the-database","title":"Changing the database","text":"<p>To change the database that pg_vectorize background worker is connected to, you can use the following SQL command:</p> <pre><code>ALTER SYSTEM SET vectorize.database_name TO 'my_new_db';\n</code></pre> <p>Then, restart Postgres.</p>"},{"location":"extension/configuration/#changing-embedding-and-llm-base-urls","title":"Changing Embedding and LLM base URLs","text":"<p>All Embedding model and LLM providers can have their base URLs changed.</p> <p>For example, if you have an OpenAI compliant embedding or LLM server (such as vLLM), running at <code>https://api.myserver.com/v1</code>, you can change the base URL with the following SQL command:</p> <pre><code>ALTER SYSTEM SET vectorize.openai_service_url TO 'https://api.myserver.com/v1';\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"extension/configuration/#changing-the-batch-job-size","title":"Changing the batch job size","text":"<p>Text data stored in Postgres is transformed into embeddings via HTTP requests made from the pg_vectorize background worker. Requests are made to the specified embedding service in batch (multiple inputs per request). The number of inputs per request is determined by the <code>vectorize.batch_size</code> GUC. This has no impact on transformations that occur during <code>vectorize.search()</code>, <code>vectorize.encode()</code> and <code>vectorize.rag()</code> which are always batch size 1 since those APIs accept only a single input (the raw text query).</p> <pre><code>ALTER SYSTEM SET vectorize.batch_size to 100;\n</code></pre>"},{"location":"extension/configuration/#available-gucs","title":"Available GUCs","text":"<p>The complete list of GUCs available for pg_vectorize are defined in extension/src/guc.rs.</p>"},{"location":"extension/api/","title":"PG Vectorize API Overview","text":"<p>pg vectorize provides tools for two closely related tasks; vector search and retrieval augmented generation (RAG), and there are APIs dedicated to both of these tasks. Vector search is an important component of RAG and the RAG APIs depend on the vector search APIs. It could be helpful to think of the vector search APIs as lower level than RAG. However, relative to Postgres's APIs, both of these vectorize APIs are very high level.</p>"},{"location":"extension/api/#importing-pre-existing-embeddings","title":"Importing Pre-existing Embeddings","text":"<p>If you have already computed embeddings for your data using a compatible model, you can import these directly into pg_vectorize using the <code>vectorize.import_embeddings</code> function:</p> <pre><code>SELECT vectorize.import_embeddings(\n    job_name =&gt; 'my_search_project',\n    src_table =&gt; 'my_source_table',\n    src_primary_key =&gt; 'id',\n    src_embeddings_col =&gt; 'embeddings'\n);\n</code></pre> <p>This function allows you to: - Import pre-computed embeddings without recomputation - Support both join and append table methods - Automatically validate embedding dimensions - Clean up any pending realtime jobs</p> <p>The embeddings must match the dimensions expected by the model specified when creating the project with <code>vectorize.table()</code>.</p>"},{"location":"extension/api/#parameters","title":"Parameters","text":"<ul> <li><code>job_name</code>: The name of your pg_vectorize project (created via <code>vectorize.table()</code>)</li> <li><code>src_table</code>: The table containing your pre-computed embeddings</li> <li><code>src_primary_key</code>: The primary key column in your source table</li> <li><code>src_embeddings_col</code>: The column containing the vector embeddings</li> </ul>"},{"location":"extension/api/#example","title":"Example","text":"<pre><code>-- First create a vectorize project\nSELECT vectorize.table(\n    job_name =&gt; 'product_search',\n    relation =&gt; 'products',\n    primary_key =&gt; 'id',\n    columns =&gt; ARRAY['description'],\n    transformer =&gt; 'sentence-transformers/all-MiniLM-L6-v2'\n);\n\n-- Then import pre-existing embeddings\nSELECT vectorize.import_embeddings(\n    job_name =&gt; 'product_search',\n    src_table =&gt; 'product_embeddings',\n    src_primary_key =&gt; 'product_id',\n    src_embeddings_col =&gt; 'embedding_vector'\n);\n</code></pre>"},{"location":"extension/api/#creating-a-table-from-existing-embeddings","title":"Creating a Table from Existing Embeddings","text":"<p>If you have pre-computed embeddings and want to create a new vectorize table from them, use <code>vectorize.table_from()</code>:</p> <pre><code>SELECT vectorize.table_from(\n    relation =&gt; 'products',\n    columns =&gt; ARRAY['description'],\n    job_name =&gt; 'product_search',\n    primary_key =&gt; 'id',\n    src_table =&gt; 'product_embeddings',\n    src_primary_key =&gt; 'product_id',\n    src_embeddings_col =&gt; 'embedding_vector',\n    transformer =&gt; 'sentence-transformers/all-MiniLM-L6-v2',\n    schedule =&gt; 'realtime'\n);\n</code></pre> <p>This function: 1. Creates the vectorize table structure 2. Imports your pre-computed embeddings 3. Sets up the specified update schedule (realtime triggers or cron job)</p> <p>The embeddings must match the dimensions of the specified transformer model.</p>"},{"location":"extension/api/#parameters_1","title":"Parameters","text":"<ul> <li><code>relation</code>: The table to create or modify</li> <li><code>columns</code>: Array of columns to generate embeddings from</li> <li><code>job_name</code>: Name for this vectorize project</li> <li><code>primary_key</code>: Primary key column in your table</li> <li><code>src_table</code>: Table containing your pre-computed embeddings</li> <li><code>src_primary_key</code>: Primary key column in your source table</li> <li><code>src_embeddings_col</code>: Column containing the vector embeddings</li> <li><code>schema</code>: Schema name (default: 'public')</li> <li><code>update_col</code>: Column tracking updates (default: 'last_updated_at')</li> <li><code>index_dist_type</code>: Index type (default: 'pgv_hnsw_cosine')</li> <li><code>transformer</code>: Model to use (default: 'sentence-transformers/all-MiniLM-L6-v2')</li> <li><code>table_method</code>: How to store embeddings (default: 'join')</li> <li><code>schedule</code>: Update schedule - 'realtime', 'manual', or cron expression (default: ' * * * ')</li> </ul> <p>This approach ensures your pre-computed embeddings are properly imported before any automatic updates are enabled.</p>"},{"location":"extension/api/rag/","title":"RAG","text":"<p>SQL API for Retrieval Augmented Generation projects.</p>"},{"location":"extension/api/rag/#initializing-a-rag-table","title":"Initializing a RAG table","text":"<p>Creates embeddings for specified data in a Postgres table. Creates index, and triggers to keep embeddings up to date.</p>"},{"location":"extension/api/rag/#vectorizetable","title":"<code>vectorize.table</code>","text":"<pre><code>vectorize.\"table\"(\n    \"relation\" TEXT,\n    \"columns\" TEXT[],\n    \"job_name\" TEXT,\n    \"primary_key\" TEXT,\n    \"schema\" TEXT DEFAULT 'public',\n    \"update_col\" TEXT DEFAULT 'last_updated_at',\n    \"transformer\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2',\n    \"index_dist_type\" vectorize.IndexDist DEFAULT 'pgv_hnsw_cosine',\n    \"table_method\" vectorize.TableMethod DEFAULT 'join',\n    \"schedule\" TEXT DEFAULT '* * * * *'\n) RETURNS TEXT\n</code></pre> Parameter Type Description relation text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to <code>last_updated_at</code> transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. index_dist_type IndexDist The name of index type to build. Defaults to 'pgv_hnsw_cosine'. table_method TableMethod <code>join</code> to store embeddings in a new table in the vectorize schema. <code>append</code> to create columns for embeddings on the source table. Defaults to <code>join</code>. schedule text Accepts a cron-like input for a cron based updates. Or <code>realtime</code> to set up a trigger. <p>Example:</p> <pre><code>select vectorize.table(\n    job_name    =&gt; 'tembo_chat',\n    \"table\"     =&gt; 'tembo_docs',\n    primary_key =&gt; 'document_name',\n    columns     =&gt; ARRAY['content'],\n    transformer =&gt; 'sentence-transformers/all-MiniLM-L12-v2'\n);\n</code></pre>"},{"location":"extension/api/rag/#query-using-rag","title":"Query using RAG","text":""},{"location":"extension/api/rag/#vectorizerag","title":"<code>vectorize.rag</code>","text":"<pre><code>vectorize.\"rag\"(\n    \"agent_name\" TEXT,\n    \"query\" TEXT,\n    \"chat_model\" TEXT DEFAULT 'openai/gpt-3.5-turbo',\n    \"task\" TEXT DEFAULT 'question_answer',\n    \"api_key\" TEXT DEFAULT NULL,\n    \"num_context\" INT DEFAULT 2,\n    \"force_trim\" bool DEFAULT false\n) RETURNS TABLE (\n    \"chat_results\" jsonb\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description job_name text Specify the name provided during vectorize.table query text The user provided query or command provided to the chat completion model. task text Specifies the name of the prompt template to use. Must exist in vectorize.prompts (prompt_type) api_key text API key for the specified chat model. If OpenAI, this value overrides the config <code>vectorize.openai_key</code> num_context int The number of context documents returned by similarity search include in the message submitted to the chat completion model force_trim bool Trims the documents provided as context, starting with the least relevant documents, such that the prompt fits into the model's context window. Defaults to false."},{"location":"extension/api/rag/#example","title":"Example","text":"<pre><code>select vectorize.rag(\n    job_name    =&gt; 'tembo_support',\n    query       =&gt; 'what are the major features from the tembo kubernetes operator?',\n    chat_model  =&gt; 'openai/gpt-3.5-turbo',\n    force_trim  =&gt; 'true'\n);\n</code></pre> <p>The response contains the contextual data used in the prompt in addition to the chat response.</p> <pre><code>{\n  \"context\": [\n    {\n      \"content\": \"\\\"Tembo Standard Stack\\\\n\\\\nThe Tembo Standard Stack is a tuned Postgres instance balance for general purpose computing. You have full control over compute, configuration, and extension installation.\\\"\",\n      \"token_ct\": 37,\n      \"record_id\": \"535\"\n    },\n    {\n      \"content\": \"\\\"Why Stacks?\\\\n\\\\nAdopting a new database adds significant complexity and costs to an engineering organization. Organizations spend a huge amount of time evaluating, benchmarking or migrating databases and setting upcomplicated pipelines keeping those databases in sync.\\\\n\\\\nMost of these use cases can be served by Postgres, thanks to its stability, feature completeness and extensibility. However, optimizing Postgres for each use case is a non-trivial task and requires domain expertise, use case understanding and deep Postgres expertise, making it hard for most developers to adopt this.\\\\n\\\\nTembo Stacks solve that problem by providing pre-built, use case optimized Postgres deployments.\\\\n\\\\nA tembo stack is a pre-built, use case specific Postgres deployment which enables you to quickly deploy specialized data services that can replace external, non-Postgres data services. They help you avoid the pains associated with adopting, operationalizing, optimizing and managing new databases.\\\\n\\\\n|Name|Replacement for|\\\\n|----|---------------|\\\\n|Data Warehouse| Snowflake, Bigquery |\\\\n|Geospatial| ESRI, Oracle |\\\\n|OLTP| Amazon RDS |\\\\n|OLAP| Snowflake, Bigquery |\\\\n|Machine Learning| MindsDB |\\\\n|Message Queue| Amazon SQS, RabbitMQ, Redis |\\\\n|Mongo Alternative on Postgres| MongoDB |\\\\n|RAG| LangChain |\\\\n|Standard| Amazon RDS |\\\\n|Vector DB| Pinecone, Weaviate |\\\\n\\\\nWe are actively working on additional Stacks. Check out the Tembo Roadmap and upvote the stacks you''d like to see next.\\\"\",\n      \"token_ct\": 336,\n      \"record_id\": \"387\"\n    }\n  ],\n  \"chat_response\": \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\"\n}\n</code></pre> <p>Filter the results to just the <code>chat_response</code>:</p> <pre><code>select vectorize.rag(\n    job_name    =&gt; 'tembo_support',\n    query       =&gt; 'what are the major features from the tembo kubernetes operator?',\n    chat_model  =&gt; 'gpt-3.5-turbo',\n    force_trim  =&gt; 'true'\n) -&gt; 'chat_response';\n</code></pre> <pre><code> \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\"\n</code></pre>"},{"location":"extension/api/search/","title":"Vector Search","text":"<p>The vector-search flow is two part; first initialize a table using <code>vectorize.table()</code>, then search the table with <code>vectorize.search()</code>.</p>"},{"location":"extension/api/search/#initialize-a-table","title":"Initialize a table","text":"<p>Initialize a table for vector search. Generates embeddings and index. Creates triggers to keep embeddings up-to-date.</p> <pre><code>vectorize.\"table\"(\n    \"relation\" TEXT,\n    \"columns\" TEXT[],\n    \"job_name\" TEXT,\n    \"primary_key\" TEXT,\n    \"schema\" TEXT DEFAULT 'public',\n    \"update_col\" TEXT DEFAULT 'last_updated_at',\n    \"transformer\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2',\n    \"index_dist_type\" vectorize.IndexDist DEFAULT 'pgv_hnsw_cosine',\n    \"table_method\" vectorize.TableMethod DEFAULT 'join',\n    \"schedule\" TEXT DEFAULT '* * * * *'\n) RETURNS TEXT\n</code></pre> Parameter Type Description relation text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to <code>last_updated_at</code> transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. index_dist_type IndexDist The name of index type to build. Defaults to 'pgv_hnsw_cosine'. table_method TableMethod <code>join</code> to store embeddings in a new table in the vectorize schema. <code>append</code> to create columns for embeddings on the source table. Defaults to <code>join</code>. schedule text Accepts a cron-like input for a cron based updates. Or <code>realtime</code> to set up a trigger."},{"location":"extension/api/search/#sentence-transformer-examples","title":"Sentence-Transformer Examples","text":""},{"location":"extension/api/search/#openai-examples","title":"OpenAI Examples","text":"<p>To use embedding model provided by OpenAI's public embedding endpoints, provide the model name into the <code>transformer</code> parameter,  and provide the OpenAI API key.</p> <p>Pass the API key into the function call via <code>args</code>.</p> <pre><code>select vectorize.table(\n    job_name    =&gt; 'product_search',\n    relation    =&gt; 'products',\n    primary_key =&gt; 'product_id',\n    columns     =&gt; ARRAY['product_name', 'description'],\n    transformer =&gt;  'openai/text-embedding-ada-002',\n    args        =&gt; '{\"api_key\": \"my-openai-key\"}'\n);\n</code></pre> <p>The API key can also be set via GUC.</p> <pre><code>ALTER SYSTEM SET vectorize.openai_key TO 'my-openai-key';\nSELECT pg_reload_conf();\n</code></pre> <p>Then call <code>vectorize.table()</code> without providing the API key.</p> <pre><code>select vectorize.table(\n    job_name    =&gt; 'product_search',\n    relation    =&gt; 'products',\n    primary_key =&gt; 'product_id',\n    columns     =&gt; ARRAY['product_name', 'description'],\n    transformer =&gt;  'openai/text-embedding-ada-002'\n);\n</code></pre>"},{"location":"extension/api/search/#search-a-table","title":"Search a table","text":"<p>Search a table initialized with <code>vectorize.table</code>. The search results are sorted in descending order according to similarity. </p> <p>The <code>query</code> is transformed to embeddings using the same <code>transformer</code> configured during <code>vectorize.table</code>.</p> <p>The <code>where_sql</code> parameter is used to apply additional filtering to the search results based on SQL conditions. </p> <pre><code>vectorize.\"search\"(\n    \"job_name\" TEXT,\n    \"query\" TEXT,\n    \"api_key\" TEXT DEFAULT NULL,\n    \"return_columns\" TEXT[] DEFAULT ARRAY['*']::text[],\n    \"num_results\" INT DEFAULT 10\n    \"where_sql\" TEXT DEFAULT NULL\n) RETURNS TABLE (\n    \"search_results\" jsonb\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description job_name text A unique name for the project. query text The user provided query or command provided to the chat completion model. api_key text API key for the specified chat model. If OpenAI, this value overrides the config <code>vectorize.openai_key</code> return_columns text[] The columns to return in the search results. Defaults to all columns. num_results int The number of results to return. Sorted in descending order according to similarity. Defaults to 10. where_sql text An optional SQL condition to filter the search results. This condition is applied after the similarity search."},{"location":"extension/api/search/#example","title":"Example","text":"<pre><code>SELECT * FROM vectorize.search(\n    job_name        =&gt; 'product_search',\n    query           =&gt; 'mobile electronic devices',\n    return_columns  =&gt; ARRAY['product_id', 'product_name'],\n    num_results     =&gt; 3\n);\n</code></pre> <pre><code>                                         search_results                                     \n\n--------------------------------------------------------------------------------------------\n----\n {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845}\n {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099}\n {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103}\n(3 rows)\n</code></pre>"},{"location":"extension/api/search/#filtering-search-results","title":"Filtering Search Results","text":"<p>The <code>where_sql</code> parameter allows to apply SQL-based filtering after performing the vector similarity search. This feature is useful when you want to narrow down the search results based on certain conditions such as <code>product category</code> or <code>price</code>.</p>"},{"location":"extension/api/search/#example_1","title":"Example","text":"<pre><code>SELECT * FROM vectorize.search(\n    job_name        =&gt; 'product_search',\n    query           =&gt; 'mobile electronic devices',\n    return_columns  =&gt; ARRAY['product_id', 'product_name'],\n    num_results     =&gt; 3,\n    where_sql       =&gt; 'product_category = ''electronics'' AND price &gt; 100'\n);\n</code></pre> <p>In the above example, the results are filtered where the <code>product_category</code> is <code>electronics</code> and the <code>price</code> is greater than 100.</p>"},{"location":"extension/api/search/#optimizing-searches-with-partial-indices","title":"Optimizing Searches with Partial Indices","text":"<p>For improving performance when using filters, you can create partial indices. This will speed up the execution of queries with frequent conditions in the <code>where_sql</code> parameter.</p>"},{"location":"extension/api/search/#example_2","title":"Example","text":"<pre><code>CREATE INDEX idx_product_price ON products (product_name) WHERE price &gt; 100;\n</code></pre> <p>This index optimizes queries that search for products where the <code>price</code> is greater than 100.</p> <p>Note: Partial indices improve performance by only indexing rows that meet the specified condition. This reduces the amount of data the database needs to scan, making queries with the same filter more efficient since only relevant rows are included in the index.</p> <p>By combining the <code>where_sql</code> filtering feature with partial indices, you can efficiently narrow down search results and improve query performance.</p>"},{"location":"extension/api/utilities/","title":"Utilities","text":""},{"location":"extension/api/utilities/#text-to-embeddings","title":"Text to Embeddings","text":"<p>Transforms a block of text to embeddings using the specified transformer.</p> <p>Requires the <code>vector-serve</code> container to be set via <code>vectorize.embedding_service_url</code>, or an OpenAI key to be set if using OpenAI embedding models.</p> <pre><code>vectorize.\"encode\"(\n    \"input\" TEXT,\n    \"model_name\" TEXT DEFAULT 'sentence-transformers/all-MiniLM-L6-v2',\n    \"api_key\" TEXT DEFAULT NULL\n) RETURNS double precision[]\n</code></pre> <p>Parameters:</p> Parameter Type Description input text Raw text to be transformed to an embedding model_name text Name of the sentence-transformer or OpenAI model to use. api_key text API key for the transformer. Defaults to NULL."},{"location":"extension/api/utilities/#example","title":"Example","text":"<pre><code>select vectorize.encode(\n    input       =&gt; 'the quick brown fox jumped over the lazy dogs',\n    model_name  =&gt; 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1'\n);\n\n{-0.2556323707103729,-0.3213586211204529 ..., -0.0951206386089325}\n</code></pre>"},{"location":"extension/api/utilities/#updating-the-database","title":"Updating the Database","text":"<p>Configure <code>vectorize</code> to run on a database other than the default <code>postgres</code>.</p> <p>Note that when making this change, it's also required to update <code>pg_cron</code> such that its corresponding background workers also connect to the appropriate database.</p>"},{"location":"extension/api/utilities/#example_1","title":"Example","text":"<pre><code>CREATE DATABASE my_new_db;\n</code></pre> <pre><code>ALTER SYSTEM SET cron.database_name TO 'my_new_db';\nALTER SYSTEM SET vectorize.database_name TO 'my_new_db';\n</code></pre> <p>Then, restart postgres to apply the changes and, if you haven't already, enable <code>vectorize</code> in your new database.</p> <pre><code>\\c my_new_db\n</code></pre> <pre><code>CREATE EXTENSION vectorize CASCADE;\n</code></pre> <pre><code>SHOW cron.database_name;\nSHOW vectorize.database_name;\n</code></pre> <pre><code> cron.database_name \n--------------------\n my_new_db\n(1 row)\n\n vectorize.database_name \n-------------------------\n my_new_db\n(1 row)\n</code></pre>"},{"location":"extension/examples/openai_embeddings/","title":"Vector Search with OpenAI","text":"<p>First you'll need an OpenAI API key.</p> <p>Set your API key as a Postgres configuration parameter.</p> <pre><code>ALTER SYSTEM SET vectorize.openai_key TO '&lt;your api key&gt;';\n\nSELECT pg_reload_conf();\n</code></pre> <p>Create an example table if it does not already exist.</p> <pre><code>CREATE TABLE products (LIKE vectorize.example_products INCLUDING ALL);\nINSERT INTO products SELECT * FROM vectorize.example_products;\n</code></pre> <p>Then create the job.  It may take some time to generate embeddings, depending on API latency.</p> <pre><code>SELECT vectorize.table(\n    job_name    =&gt; 'product_search_openai',\n    relation    =&gt; 'products',\n    primary_key =&gt; 'product_id',\n    columns     =&gt; ARRAY['product_name', 'description'],\n    transformer =&gt; 'openai/text-embedding-ada-002'\n);\n</code></pre> <p>To search the table, use the <code>vectorize.search</code> function.</p> <pre><code>SELECT * FROM vectorize.search(\n    job_name        =&gt; 'product_search_openai',\n    query           =&gt; 'accessories for mobile devices',\n    return_columns  =&gt; ARRAY['product_id', 'product_name'],\n    num_results     =&gt; 3\n);\n</code></pre> <pre><code>                                         search_results                                     \n\n--------------------------------------------------------------------------------------------\n----\n {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845}\n {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099}\n {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103}\n(3 rows)\n</code></pre>"},{"location":"extension/examples/scheduling/","title":"Scheduling Embedding Updates","text":"<p>When the source text data is updated, how and when the embeddings are updated is determined by the value set to the <code>schedule</code> parameter in <code>vectorize.table</code>s.</p> <p>The default behavior is <code>schedule =&gt; '* * * * *'</code>, which means the background worker process checks for changes every minute, and updates the embeddings accordingly. This method requires setting the <code>updated_at_col</code> value to point to a colum on the table indicating the time that the input text columns were last changed. <code>schedule</code> can be set to any cron-like value.</p> <p>Alternatively, <code>schedule =&gt; 'realtime</code> creates triggers on the source table and updates embeddings anytime new records are inserted to the source table or existing records are updated.</p> <p>Statements below would will result in new embeddings being generated either immediately (<code>schedule =&gt; 'realtime'</code>) or within the cron schedule set in the <code>schedule</code> parameter.</p> <pre><code>INSERT INTO products (product_id, product_name, description, product_category, price)\nVALUES (12345, 'pizza', 'dish of Italian origin consisting of a flattened disk of bread', 'food', 5.99);\n\nUPDATE products\nSET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting'\nWHERE product_name = 'Hammock';\n</code></pre>"},{"location":"extension/examples/sentence_transformers/","title":"Sentence Transformers","text":"<p>Setup a products table. Copy from the example data provided by the extension.</p> <p>Ensure <code>vectorize.embedding_svc_url</code> is set to the URL of the vector-serve container.</p> <p>If you're running this example using the docker-compose.yaml file from this repo, it should look like this:</p> <pre><code>SHOW vectorize.embedding_service_url;\n</code></pre> <pre><code>    vectorize.embedding_service_url     \n----------------------------------------\n http://vector-serve:3000/v1/embeddings\n(1 row)\n</code></pre> <p>If you are not running in docker, then you will need to change the url to the appropriate location.  If that is localhost, it would look like this;</p> <pre><code>ALTER SYSTEM SET vectorize.embedding_svc_url TO 'http://localhost:3000/v1/embeddings';\n</code></pre> <p>Then reload Postgres configurations:</p> <pre><code>SELECT pg_reload_conf();\n</code></pre> <p>Create an example table if it does not already exist.</p> <pre><code>CREATE TABLE products (LIKE vectorize.example_products INCLUDING ALL);\nINSERT INTO products SELECT * FROM vectorize.example_products;\n</code></pre> <pre><code>SELECT * FROM products limit 2;\n</code></pre> <pre><code> product_id | product_name |                      description                       |        last_updated_at        \n------------+--------------+--------------------------------------------------------+-------------------------------\n          1 | Pencil       | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05\n          2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics    | 2023-07-26 17:20:43.639351-05\n</code></pre> <p>Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description).</p> <pre><code>SELECT vectorize.table(\n    job_name    =&gt; 'product_search_hf',\n    relation    =&gt; 'products',\n    primary_key =&gt; 'product_id',\n    columns     =&gt; ARRAY['product_name', 'description'],\n    transformer =&gt; 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1',\n    scheduler   =&gt; 'realtime'\n);\n</code></pre> <p>This adds a new column to your table, in our case it is named <code>product_search_embeddings</code>, then populates that data with the transformed embeddings from the <code>product_name</code> and <code>description</code> columns.</p> <p>Then search,</p> <pre><code>SELECT * FROM vectorize.search(\n    job_name        =&gt; 'product_search_hf',\n    query           =&gt; 'accessories for mobile devices',\n    return_columns  =&gt; ARRAY['product_id', 'product_name'],\n    num_results     =&gt; 3\n);\n\n                                       search_results                                        \n---------------------------------------------------------------------------------------------\n {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8147814132322894}\n {\"product_id\": 6, \"product_name\": \"Backpack\", \"similarity_score\": 0.7743061352550308}\n {\"product_id\": 11, \"product_name\": \"Stylus Pen\", \"similarity_score\": 0.7709902653575383}\n</code></pre>"},{"location":"extension/models/","title":"Supported Transformers and Generative Models","text":"<p>pg_vectorize provides hooks into two types of models; <code>text-to-embedding</code> transformer models and <code>text-generation</code> models.  Whether a model is a text-to-embedding transformer or a text generation model, the models are always referenced from SQL using the following syntax:</p> <p><code>${provider}/${model-name}</code></p> <p>A few illustrative examples:</p> <ul> <li><code>openai/text-embedding-ada-002</code> is one of OpenAI's earliest embedding models</li> <li><code>openai/gpt-3.5-turbo-instruct</code> is a text generation model from OpenAI.</li> <li><code>ollama/wizardlm2:7b</code> is a language model hosted in Ollama and developed by MicrosoftAI.</li> <li><code>sentence-transformers/all-MiniLM-L12-v2</code> is a text-to-embedding model from SentenceTransformers.</li> </ul>"},{"location":"extension/models/#text-to-embedding-models","title":"Text-to-Embedding Models","text":"<p>pg_vectorize provides hooks into the following tex-to-embedding models:</p> <ul> <li>OpenAI (public API)</li> <li>SentenceTransformers (self-hosted)</li> </ul> <p>The transformer model that you want to be used is specified in a parameter in various functions in this project,</p> <p>For example, the <code>sentence-transformers</code> provider has a model named <code>all-MiniLM-L12-v2</code>.  The model name is <code>sentence-transformers/all-MiniLM-L12-v2</code>. To use openai's <code>text-embedding-ada-002</code>,  the model name is <code>openai/text-embedding-ada-002</code>.</p>"},{"location":"extension/models/#sentencetransformers","title":"SentenceTransformers","text":"<p>SentenceTransformers is a Python library for computing text embeddings.  pg_vectorize provides a container image that implements the SentenceTransformer library beyind a REST API.  The container image is pre-built with <code>sentence-transformers/all-MiniLM-L12-v2</code> pre-cached.  Models that are not pre-cached will be downloaded on first use and cached for subsequent use.</p> <p>When calling the model server from Postgres, the url to the model server must first be set in the <code>vectorize.embedding_service_url</code> configuration parameter.  Assuming the model server is running on the same host as Postgres, you would set the following:</p> <pre><code>ALTER SYSTEM SET vectorize.embedding_service_url TO 'http://localhost:3000/v1/embeddings';\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"extension/models/#running-the-model-server","title":"Running the model server","text":"<p>You can run this model server locally by executing</p> <pre><code>docker compose up vector-serve -d\n</code></pre> <p>Then call it with simple curl commands:</p>"},{"location":"extension/models/#calling-with-curl","title":"Calling with curl","text":"<pre><code>curl -X POST http://localhost:3000/v1/embeddings \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"input\": [\"solar powered mobile electronics accessories without screens\"],\n   \"model\": \"sentence-transformers/all-MiniLM-L12-v2\"}'\n</code></pre> <pre><code>{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.07903402298688889,\n        0.028912536799907684,\n        -0.018827738240361214,\n        -0.013423092663288116,\n        -0.06503172218799591,\n          ....384 total elements\n      ],\n      \"index\": 0\n    }\n  ],\n  \"model\": \"all-MiniLM-L12-v2\"\n}\n</code></pre> <p>We can change the model name to any of the models supported by SentenceTransformers, and it will be downloaded on-the-fly.</p> <pre><code>curl -X POST http://localhost:3000/v1/embeddings \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"input\": [\"solar powered mobile electronics accessories without screens\"],\n   \"model\": \"sentence-transformers/sentence-t5-base\"}'\n</code></pre> <pre><code>{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.07903402298688889,\n        0.028912536799907684,\n        -0.018827738240361214,\n        -0.013423092663288116,\n        -0.06503172218799591,\n          ....384 total elements\n      ],\n      \"index\": 0\n    }\n  ],\n  \"model\": \"sentence-transformers/sentence-t5-base\"\n}\n</code></pre>"},{"location":"extension/models/#calling-with-sql","title":"Calling with SQL","text":"<p>We can also call the model server from SQL using the <code>pg_vectorize.transform_embeddings</code> function.</p> <p>Model name support rules apply the same.</p> <pre><code>select vectorize.transform_embeddings(\n    input       =&gt; 'the quick brown fox jumped over the lazy dogs',\n    model_name  =&gt; 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1'\n);\n</code></pre> <pre><code>{-0.2556323707103729,-0.3213586211204529 ..., -0.0951206386089325}\n</code></pre>"},{"location":"extension/models/#openai","title":"OpenAI","text":"<p>OpenAI embedding models are hosted by OpenAI's public API.  You just need to have an API key of your own, and it can be set with:</p> <pre><code>ALTER SYSTEM SET vectorize.openai_key TO '&lt;your api key&gt;';\n\nSELECT pg_reload_conf();\n</code></pre> <p>To call the <code>text-embedding-ada-002</code> from OpenAI:</p> <pre><code>select vectorize.transform_embeddings(\n    input       =&gt; 'the quick brown fox jumped over the lazy dogs',\n    model_name  =&gt; 'openai/text-embedding-ada-002'\n);\n</code></pre> <p>To call <code>text-embedding-3-large</code></p> <pre><code>select vectorize.transform_embeddings(\n    input =&gt; 'the quick brown fox jumped over the lazy dogs',\n    model_name =&gt; 'openai/text-embedding-3-large'\n);\n</code></pre>"},{"location":"extension/models/#text-generation-models","title":"Text Generation Models","text":"<p>pg_vectorize provides hooks into the following text generation models:</p> <ul> <li>OpenAI (public API)</li> <li>Ollama (self-hosted)</li> </ul>"},{"location":"extension/models/#ollama-generative-models","title":"Ollama Generative Models","text":"<p>To run the self-hosted Ollama models, you must first start the model server:</p> <pre><code>docker compose up ollama-serve -d\n</code></pre> <p>This starts an Ollama server pre-loaded with the <code>wizardlm2:7b</code> model.</p>"},{"location":"extension/models/#calling-with-curl_1","title":"Calling with <code>curl</code>","text":"<p>Once the Ollama server is running, you can call it directly with <code>curl</code>:</p> <pre><code>curl http://localhost:3001/api/generate -d '{\n  \"model\": \"wizardlm2:7b\",\n  \"prompt\": \"What is Postgres?\"\n}'\n</code></pre>"},{"location":"extension/models/#calling-with-sql_1","title":"Calling with SQL","text":"<p>First set the url to the Ollama server:</p> <pre><code>ALTER SYSTEM set vectorize.ollama_service_url TO 'http://localhost:3001`;\nSELECT pg_reload_conf();\n</code></pre> <p>The text-generation models are available as part of the RAG API.  To call the models provided by the self-hosted Ollama container,  pass the model name into the <code>chat_model</code> parameter.</p> <pre><code>SELECT vectorize.rag(\n    job_name    =&gt; 'product_chat',\n    query       =&gt; 'What is a pencil?',\n    chat_model  =&gt; 'ollama/wizardlm2:7b'\n);\n</code></pre>"},{"location":"extension/models/#loading-new-ollama-models","title":"Loading new Ollama models","text":"<p>While Ollama server comes preloaded with <code>wizardlm2:7b</code>, we can load and model supported by Ollama by calling the <code>/api/pull</code> endpoint.  The service is compatible with all models available in the Ollama library.</p> <p>To pull Llama 3:</p> <pre><code>curl http://localhost:3001/api/pull -d '{\n  \"name\": \"llama3\"\n}'\n</code></pre> <p>Then use that model in your RAG application:</p> <pre><code>SELECT vectorize.rag(\n    job_name    =&gt; 'product_chat',\n    query       =&gt; 'What is a pencil?'\n    chat_model  =&gt; 'ollama/llama3'\n);\n</code></pre>"},{"location":"server/","title":"Index","text":""},{"location":"server/#server","title":"Server","text":"<p>This section documents the HTTP server that provides the vectorize job management and search API.</p> <p>See the repository <code>server/README.md</code> for a higher-level getting started guide and examples.</p> <p>Available pages:</p> <ul> <li>API reference: <code>docs/server/api/table.md</code> - Initialize a vectorize job (POST /api/v1/table)</li> <li>API reference: <code>docs/server/api/search.md</code> - Search the indexed data (GET /api/v1/search)</li> </ul> <p>You can run the server locally using the instructions in <code>server/README.md</code>. The examples on the API pages below assume the server is running on http://localhost:8080.</p>"},{"location":"server/api/search/","title":"Search","text":""},{"location":"server/api/search/#get-apiv1search","title":"GET /api/v1/search","text":"<p>Perform a hybrid semantic + full-text search against a previously initialized vectorize job.</p> <p>URL</p> <p>/api/v1/search</p> <p>Method</p> <p>GET</p> <p>Query parameters</p> <ul> <li>job_name (string) - required</li> <li>Name of the vectorize job to search. This identifies the table, schema, model and other job configuration.</li> <li>query (string) - required</li> <li>The user's search query string.</li> <li>limit (int) - optional, default: 10</li> <li>Maximum number of results to return.</li> <li>window_size (int) - optional, default: 5 * limit</li> <li>Internal window size used by the hybrid search algorithm.</li> <li>rrf_k (float) - optional, default: 60.0</li> <li>Reciprocal Rank Fusion param used by the hybrid ranking.</li> <li>semantic_wt (float) - optional, default: 1.0</li> <li>Weight applied to the semantic score.</li> <li>fts_wt (float) - optional, default: 1.0</li> <li>Weight applied to the full-text-search score.</li> <li>filters (object) - optional</li> <li>Additional filters are accepted as query params and are passed as typed filter values to the query builder. Filters are provided as URL query parameters and will be parsed into a map of keys to values. The server validates keys and raw string values for safety.</li> </ul> <p>Notes on filters</p> <p>Filters are supplied as query parameters and the server will parse them into a BTreeMap of filter keys and typed values. The server validates string inputs to avoid SQL injection; only the job is allowed to specify table/column names on job creation. See the source for details about accepted filter types.</p> <p>Example request</p> <pre><code>curl -G \"http://localhost:8080/api/v1/search\" \\\n  --data-urlencode \"job_name=my_job\" \\\n  --data-urlencode \"query=camping gear\" \\\n  --data-urlencode \"limit=2\"\n</code></pre> <p>Example response (200)</p> <p>The endpoint returns an array of JSON objects. The exact shape depends on the columns selected by the job (server uses <code>SELECT *</code> for results), plus additional ranking fields. Example returned item:</p> <pre><code>[\n  {\n    \"product_id\": 39,\n    \"product_name\": \"Hammock\",\n    \"description\": \"Sling made of fabric or netting, suspended between two points for relaxation\",\n    \"product_category\": \"outdoor\",\n    \"price\": 40.0,\n    \"updated_at\": \"2025-06-25T19:57:22.410561+00:00\",\n    \"semantic_rank\": 1,\n    \"similarity_score\": 0.3192296909597241,\n    \"rrf_score\": 0.01639344262295082,\n    \"fts_rank\": null\n  }\n]\n</code></pre> <p>Errors</p> <ul> <li>400 / InvalidRequest - missing or invalid parameters</li> <li>404 / NotFound - job not found</li> <li>500 / InternalServerError - other server-side errors</li> </ul>"},{"location":"server/api/table/","title":"Table","text":""},{"location":"server/api/table/#post-apiv1table","title":"POST /api/v1/table","text":"<p>Create (initialize) a vectorize job that will generate embeddings and keep them in sync for a Postgres table.</p> <p>URL</p> <p>/api/v1/table</p> <p>Method</p> <p>POST</p> <p>Request body (JSON)</p> <p>The request expects a <code>VectorizeJob</code> JSON object. Required fields below are inferred from the server code and core types used by the server.</p> <ul> <li>job_name: string</li> <li>A unique name for this vectorize job.</li> <li>src_table: string</li> <li>Table name that contains the data to index.</li> <li>src_schema: string</li> <li>Schema name where the table lives (e.g., <code>public</code>).</li> <li>src_columns: array[string]</li> <li>List of columns to include when building the embeddings (for example: <code>[\"product_name\", \"description\"]</code>).</li> <li>primary_key: string</li> <li>Column name of the primary key for the source table.</li> <li>update_time_col: string</li> <li>Column name that contains last-updated timestamps for rows. NOTE: the server enforces this column is of type <code>timestamp with time zone</code>.</li> <li>model: string</li> <li>Embedding model identifier (e.g. <code>sentence-transformers/all-MiniLM-L6-v2</code> or other provider model string supported by the transformers/provider layer).</li> </ul> <p>Example request</p> <pre><code>curl -X POST http://localhost:8080/api/v1/table -d '{\n  \"job_name\": \"my_job\",\n  \"src_table\": \"my_products\",\n  \"src_schema\": \"public\",\n  \"src_columns\": [\"product_name\", \"description\"],\n  \"primary_key\": \"product_id\",\n  \"update_time_col\": \"updated_at\",\n  \"model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n}' -H \"Content-Type: application/json\"\n</code></pre> <p>Validation and behavior</p> <ul> <li>The server validates that <code>update_time_col</code> exists on the table and its data type is <code>timestamp with time zone</code>. If not, the server will return an error.</li> <li>On success the server initializes job metadata in Postgres and returns a JSON object with the job id.</li> </ul> <p>Success response (200)</p> <pre><code>{\n  \"id\": \"&lt;uuid&gt;\"\n}\n</code></pre> <p>Errors</p> <ul> <li>400 / InvalidRequest - malformed payload or validation failed (e.g., wrong timestamp type)</li> <li>404 / NotFound - referenced table/column or objects not found</li> <li>500 / InternalServerError - other server-side errors</li> </ul>"}]}